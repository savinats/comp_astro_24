{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savinats/comp_astro_24/blob/main/Machine_learning_theoretical_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7DEA3aKgOF_"
      },
      "source": [
        "# Linear Regression with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXFSLI0fgOGL"
      },
      "source": [
        "We are using sciokit-learn to generate the regression dataset between salary and experience with 100 samples and 1 feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DoFNX6LgOGN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Creating a function f(X) with a slope of -5\n",
        "x = torch.arange(0, 10, 0.1).view(-1, 1)\n",
        "y = 2 * x\n",
        "\n",
        "plt.plot(x, y, '.')\n",
        "plt.xlabel('Years of experience')\n",
        "plt.ylabel('Salary per month ($k)')\n",
        "plt.title('The relationship between experience & salary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iy-nQwjgOGV"
      },
      "source": [
        "Add a Gaussian Noise to the curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuX89T9JgOGX"
      },
      "outputs": [],
      "source": [
        "# Adding Gaussian noise to the function f(X) and saving it in Y\n",
        "Y = y + 1.2 * torch.randn(x.size())\n",
        "\n",
        "plt.plot(x, Y, '.')\n",
        "plt.xlabel('Years of experience')\n",
        "plt.ylabel('Salary per month ($k)')\n",
        "plt.title('The relationship between experience & salary')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAaMDiu8gOGZ"
      },
      "source": [
        "Split the set into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD630o_NgOGd"
      },
      "outputs": [],
      "source": [
        "xTrain, xTest, yTrain, yTest = train_test_split(x, Y, test_size=0.3, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z20vnhKngOGg"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIXFlitfgOGh"
      },
      "outputs": [],
      "source": [
        "# defining the function for forward pass for prediction\n",
        "# def forward(x):\n",
        "#     return w * x\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "model = LinearRegression(1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXbuLzlDgOGj"
      },
      "source": [
        "Define the Loss class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x-BfjSPgOGk"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEBdKpdIgOGl"
      },
      "source": [
        "We use the gradient descent optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9FQyjwogOGm"
      },
      "outputs": [],
      "source": [
        "learningRate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJTRMYRUgOGq"
      },
      "source": [
        "Let's train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJpdI22WgOGr"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    epoch += 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(xTrain)\n",
        "\n",
        "    loss = criterion(outputs, yTrain)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch: {} | Loss: {}'.format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jToZWt59gOGs"
      },
      "source": [
        "Let's visualize the Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjnZeViQgOGs"
      },
      "outputs": [],
      "source": [
        "# Visualizing the loss data\n",
        "plt.plot(range(epochs), losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVJYDwcgOGt"
      },
      "source": [
        "The train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKqDELE4gOGu"
      },
      "outputs": [],
      "source": [
        "# Visualizing the trained model\n",
        "def plotModel():\n",
        "    plt.title('Trained Model')\n",
        "    plt.xlabel('Years of experience')\n",
        "    plt.ylabel('Salary per month ($k)')\n",
        "\n",
        "    weight, bias = model.parameters()\n",
        "\n",
        "    w = weight[0][0].item()\n",
        "    b = bias[0].item()\n",
        "\n",
        "    X1 = xTrain\n",
        "    Y1 = w * X1 + b\n",
        "\n",
        "    plt.plot(X1, Y1, 'g')\n",
        "    plt.scatter(xTrain, yTrain)\n",
        "    plt.show()\n",
        "\n",
        "plotModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w-CWlnegOGv"
      },
      "source": [
        "# Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBSaAB0XgOGv"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import torch\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TOuhBrggOGw"
      },
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
        "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])\n",
        "_ = ax.legend(\n",
        "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMr_JThEgOGw"
      },
      "outputs": [],
      "source": [
        "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
        "%matplotlib widget\n",
        "import mpl_toolkits.mplot3d  # noqa: F401\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "fig = plt.figure(1, figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n",
        "\n",
        "X_reduced = PCA(n_components=3).fit_transform(iris.data)\n",
        "ax.scatter(\n",
        "    X_reduced[:, 0],\n",
        "    X_reduced[:, 1],\n",
        "    X_reduced[:, 2],\n",
        "    c=iris.target,\n",
        "    s=40,\n",
        ")\n",
        "\n",
        "ax.set_title(\"First three PCA dimensions\")\n",
        "ax.set_xlabel(\"1st Eigenvector\")\n",
        "ax.xaxis.set_ticklabels([])\n",
        "ax.set_ylabel(\"2nd Eigenvector\")\n",
        "ax.yaxis.set_ticklabels([])\n",
        "ax.set_zlabel(\"3rd Eigenvector\")\n",
        "ax.zaxis.set_ticklabels([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3vzwUDFgOGx"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tiris.target,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\trandom_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mYLiJGNgOGx"
      },
      "outputs": [],
      "source": [
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train).float()\n",
        "X_test = torch.tensor(X_test).float()\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "\n",
        "# Normalize the features\n",
        "mean = X_train.mean(dim=0)\n",
        "std = X_train.std(dim=0)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wn7aT8jgOGy"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = torch.nn.Sequential(\n",
        "\ttorch.nn.Linear(in_features = 4, out_features =3),\n",
        "\ttorch.nn.Softmax(dim=1)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5WRqlZegOGy"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "num_epochs = 10000\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "\t# Forward pass\n",
        "\ty_pred = model(X_train)\n",
        "\tloss = criterion(y_pred, y_train)\n",
        "\n",
        "\t# Backward pass and optimization\n",
        "\toptimizer.zero_grad()\n",
        "\tloss.backward()\n",
        "\toptimizer.step()\n",
        "\tlosses.append(loss.item())\n",
        "\n",
        "\t# Print the loss every 100 epochs\n",
        "\tif (epoch+1) % 100 == 0:\n",
        "\t\tprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mellGCvgOGz"
      },
      "outputs": [],
      "source": [
        "# Visualizing the loss data\n",
        "plt.figure()\n",
        "plt.plot(range(num_epochs), losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFafinK7gOG0"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "\ty_pred = model(X_test)\n",
        "\t_, predicted = torch.max(y_pred, dim=1)\n",
        "\taccuracy = (predicted == y_test).float().mean()\n",
        "\tprint(f'Test Accuracy: {accuracy.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acLX6m_LgOG0"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "Y = iris.target\n",
        "\n",
        "# Create an instance of Logistic Regression Classifier and fit the data.\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X, Y)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(4, 3))\n",
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    logreg,\n",
        "    X,\n",
        "    cmap=plt.cm.Paired,\n",
        "    ax=ax,\n",
        "    response_method=\"predict\",\n",
        "    plot_method=\"pcolormesh\",\n",
        "    shading=\"auto\",\n",
        "    xlabel=\"Sepal length\",\n",
        "    ylabel=\"Sepal width\",\n",
        "    eps=0.5,\n",
        ")\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n",
        "\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cHWyDCHgOG1"
      },
      "source": [
        "# Support Vector Machine Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcC_Sq0TgOG1"
      },
      "outputs": [],
      "source": [
        "# Authors: Clay Woolam   <clay@woolam.org>\n",
        "#          Oliver Rausch <rauscho@ethz.ch>\n",
        "# License: BSD\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# step size in the mesh\n",
        "h = 0.02\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "y_rand = rng.rand(y.shape[0])\n",
        "y_30 = np.copy(y)\n",
        "y_30[y_rand < 0.3] = -1  # set random samples to be unlabeled\n",
        "y_50 = np.copy(y)\n",
        "y_50[y_rand < 0.5] = -1\n",
        "# we create an instance of SVM and fit out data. We do not scale our\n",
        "# data since we want to plot the support vectors\n",
        "ls30 = (LabelSpreading().fit(X, y_30), y_30, \"Label Spreading 30% data\")\n",
        "ls50 = (LabelSpreading().fit(X, y_50), y_50, \"Label Spreading 50% data\")\n",
        "ls100 = (LabelSpreading().fit(X, y), y, \"Label Spreading 100% data\")\n",
        "\n",
        "# the base classifier for self-training is identical to the SVC\n",
        "base_classifier = SVC(kernel=\"rbf\", gamma=0.5, probability=True)\n",
        "st30 = (\n",
        "    SelfTrainingClassifier(base_classifier).fit(X, y_30),\n",
        "    y_30,\n",
        "    \"Self-training 30% data\",\n",
        ")\n",
        "st50 = (\n",
        "    SelfTrainingClassifier(base_classifier).fit(X, y_50),\n",
        "    y_50,\n",
        "    \"Self-training 50% data\",\n",
        ")\n",
        "\n",
        "rbf_svc = (SVC(kernel=\"rbf\", gamma=0.5).fit(X, y), y, \"SVC with rbf kernel\")\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "color_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}\n",
        "\n",
        "classifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)\n",
        "plt.figure()\n",
        "for i, (clf, y_train, title) in enumerate(classifiers):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    plt.subplot(3, 2, i + 1)\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = [color_map[y] for y in y_train]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors=\"black\")\n",
        "\n",
        "    plt.title(title)\n",
        "\n",
        "plt.suptitle(\"Unlabeled points are colored white\", y=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X2tpVfZgOG2"
      },
      "source": [
        "# Artificial Neural Network Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAqV05nzgOG4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader # loads data in batches\n",
        "from torchvision import datasets # load MNIST\n",
        "import torchvision.transforms as T # transformers for computer vision\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkILtW1xgOG4"
      },
      "outputs": [],
      "source": [
        "mytransform = T.ToTensor() # image (3D array) to Tensor\n",
        "\n",
        "train_data = datasets.MNIST(root = './', download=True, train = True, transform = mytransform)\n",
        "test_data = datasets.MNIST(root = './', download=True, train = False, transform = mytransform)\n",
        "\n",
        "img, label = train_data[0]\n",
        "img.shape # returns a Tensor of Size 1,28,28\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGMuKQENgOG5"
      },
      "outputs": [],
      "source": [
        "# We could simply plot the tensor\n",
        "plt.imshow(img.reshape(28,28), cmap = 'gist_yarg'); # gist_yarg plots inverse of W&B\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYXc9G9ngOG5"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(101)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = 100, shuffle=True)\n",
        "# the test loader can be bigger and doesn't need to be shuffled\n",
        "test_loader =  DataLoader(test_data,  batch_size = 500, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZgczhpngOG6"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Loader:\n",
        "    \"\"\"\n",
        "    Custom class to accomodate train and test loader iterators\n",
        "    \"\"\"\n",
        "    train: DataLoader  # train DataLoader iterable object\n",
        "    test:  DataLoader  # test DataLoader iterable object\n",
        "\n",
        "myloader = Loader(train = train_loader, test = test_loader)\n",
        "print(myloader)\n",
        "\n",
        "# Plot 10 images\n",
        "for img, label in myloader.train:\n",
        "    break # we run only one iteration , after that we break, we need it jus to see the shape\n",
        "img.shape # bz, ch, W H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isdvKbhagOG7"
      },
      "outputs": [],
      "source": [
        "myimages = img[:50].numpy() # we now obtain NumPy arrays\n",
        "print(f\"My Images shape: \", myimages.shape)\n",
        "\n",
        "print(\"Transpose Images shape: \", myimages[0].transpose(1,2,0).shape) # height, width, channel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvE1ozlXgOHI"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 5, ncols = 10, figsize=(8,4), subplot_kw={'xticks': [], 'yticks': []})\n",
        "for row in range(0,5):\n",
        "    for col in range(0,10):\n",
        "        myid = (10*row) + col # (ncols*rows) + cols\n",
        "\n",
        "        ax[row,col].imshow( myimages[myid].transpose(1,2,0), cmap = 'gist_yarg' ) # W,H,C\n",
        "        ax[row,col].axis('off')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y1SpHBtgOHI"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple multilayer perceptron network\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features = 784, out_features=10):\n",
        "        \"\"\"\n",
        "        * 784 input layers\n",
        "        * 2 hiden layers of 120 and 84 neurons respectively\n",
        "        * 10 output layer\n",
        "        \"\"\"\n",
        "        super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, out_features)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = F.relu(self.fc1(X)) # Rectified Linear Unit (ReLU)\n",
        "        X = F.relu(self.fc2(X)) # Rectified Linear Unit (ReLU)\n",
        "        X = self.fc3(X)\n",
        "\n",
        "        return F.log_softmax(X, dim = 1) # multi-class classification, the sum of all probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rTdxG4wgOHJ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(101)\n",
        "\n",
        "mymodel = MultilayerPerceptron() # default params are in_features = 784, out_features=10\n",
        "mymodel # check topology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYbzWCcBgOHJ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(mymodel.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90rYHlUfgOHK"
      },
      "outputs": [],
      "source": [
        "params = [p.numel() for p in mymodel.parameters() if p.requires_grad]\n",
        "np.sum(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glVBdGvogOHK"
      },
      "outputs": [],
      "source": [
        "# Plot 10 images\n",
        "myiter = iter(myloader.train)\n",
        "img, label = myiter.__next__() # only one iteration\n",
        "img.shape # batch_size, channel, Height, Width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFCn1T8BgOHL"
      },
      "outputs": [],
      "source": [
        "img.view(100,-1).shape # 100 batches of 784 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYGiOh8BgOHL"
      },
      "outputs": [],
      "source": [
        "y_pred = mymodel( img.view(100,-1) )\n",
        "y_pred.shape # 100 x 10, meaning for every batch (100) we obtain  (10 probabilities) predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgk08Jr-gOHL"
      },
      "outputs": [],
      "source": [
        "val, idx = torch.max(y_pred, dim=1) # dim 1 is for the output\n",
        "idx # indices == predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuv_lpY8gOHM"
      },
      "outputs": [],
      "source": [
        "# tracking variables\n",
        "\n",
        "class Loss:\n",
        "    \"\"\" Class to monitor train and test lost\"\"\"\n",
        "    train: list = []\n",
        "    test: list = []\n",
        "\n",
        "\n",
        "class Accuracy:\n",
        "    \"\"\" Class to monitor train and test accuracy\"\"\"\n",
        "    train: list = []\n",
        "    test: list = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObsM_VrrgOHM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # we set the number of True positives to zero in every epoch\n",
        "    train_corr = 0\n",
        "    test_corr = 0\n",
        "\n",
        "    # training: we run the train dataset in batches of 100 images, meaning that run the loader 600 times (600 x 100 = 60,000 images)\n",
        "    for batch, (img, label) in enumerate(myloader.train):\n",
        "        batch +=1\n",
        "        y_pred = mymodel( img.view(100,-1) ) # batch size for train is 100\n",
        "        loss = criterion(y_pred, label)\n",
        "\n",
        "        # last 10-layer neurons into one result\n",
        "        _, prediction = torch.max(y_pred,dim = 1)\n",
        "        train_corr += (prediction == label).sum() # sum of correct predictions\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # monitor loss and accuracy every 200 train batches of 100 images ( 3 times per epoch)\n",
        "        if batch%200 == 0:\n",
        "            acc = 100 * (train_corr.item() / (batch*100) ) # correct predictions divided by images in batches\n",
        "            print( f'Epoch:{epoch:2d} batch: {batch:2d} loss: {loss.item():4.4f} Accuracy: {acc:4.4f} %' )\n",
        "\n",
        "    Loss.train.append( loss.item() ) # store loss at the end of epoch\n",
        "    accuracy = 100 * (train_corr.item() / (batch*100) ) # accuracy is 100 x train_corr.item()/60_000\n",
        "    Accuracy.train.append( accuracy ) # store accuracy at the end of epoch\n",
        "\n",
        "\n",
        "    # validation (test). Here we run batches of 500 images, the test loader runs 120 times (120 x 500 = 60,000)\n",
        "    with torch.no_grad():\n",
        "        for batch, (img, label) in enumerate(myloader.test):\n",
        "            batch +=1\n",
        "            y_val = mymodel( img.view(500,-1) ) # batch size for test is 500\n",
        "            _, predicted = torch.max( y_val, dim = 1)\n",
        "            test_corr += (predicted == label).sum()\n",
        "\n",
        "    loss = criterion( y_val,label )\n",
        "    Loss.test.append( loss.item() )\n",
        "    accuracy = 100 * (test_corr.item()/ (batch*500) ) # accuracy is 100 x train_corr.item()/60,000\n",
        "    Accuracy.test.append( accuracy )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx--kUowgOHM"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
        "ax[0].plot(Loss.train, label = 'Training')\n",
        "ax[0].plot(Loss.test, label='test/validation')\n",
        "ax[0].set_ylabel('Loss', fontsize=16)\n",
        "\n",
        "\n",
        "ax[1].plot(Accuracy.train, label = 'Training')\n",
        "ax[1].plot(Accuracy.test, label='test/validation')\n",
        "ax[1].set_yticks(range(85,110,5))\n",
        "ax[1].axvline(x=2, color='gray', linestyle=':')\n",
        "ax[1].axhline(y=100, color='gray', linestyle=':')\n",
        "ax[1].set_ylabel('Accuracy (%)', fontsize=16)\n",
        "\n",
        "for myax in ax:\n",
        "    myax.set_xlabel('Epoch', fontsize=16)\n",
        "    myax.set_xticks(range(epochs))\n",
        "    myax.legend(frameon=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QPf0c5BgOHN"
      },
      "outputs": [],
      "source": [
        "test_loader =  DataLoader(test_data,  batch_size = 10_000, shuffle=False) # the whole test is 10,000 images\n",
        "myiter = iter(test_loader)\n",
        "img, label = myiter.__next__()\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX4rS8T6gOHO"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "\n",
        "    for X, y_label in test_loader:\n",
        "            y_val = mymodel( X.view(X.shape[0],-1) ) # flatten\n",
        "            _, predicted = torch.max( y_val, dim = 1)\n",
        "            correct += (predicted == y_label).sum()\n",
        "\n",
        "print(f'Test accuracy: = {correct.item()*100/(len(test_data)):2.4f} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk1KRQr6gOHP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix  # for evaluating results\n",
        "confusion_array = confusion_matrix(y_true = y_label,y_pred = predicted)\n",
        "confusion_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Xmah7igOHQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "df_cm = pd.DataFrame(confusion_array, range(10), range(10))\n",
        "plt.figure(figsize=(10,7))\n",
        "sn.set_theme(font_scale=1.4) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}) # font size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGuv1p3gOHQ"
      },
      "source": [
        "# Classifier Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPUdLQJcgOHR"
      },
      "outputs": [],
      "source": [
        "# Code source: Gaël Varoquaux\n",
        "#              Andreas Müller\n",
        "# Modified for documentation by Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.datasets import make_circles, make_classification, make_moons\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Linear SVM\",\n",
        "    \"RBF SVM\",\n",
        "    \"Gaussian Process\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net\",\n",
        "    \"AdaBoost\",\n",
        "    \"Naive Bayes\",\n",
        "    \"QDA\",\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
        "    SVC(gamma=2, C=1, random_state=42),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
        "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    RandomForestClassifier(\n",
        "        max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
        "    ),\n",
        "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
        "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "]\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
        ")\n",
        "rng = np.random.RandomState(2)\n",
        "X += 2 * rng.uniform(size=X.shape)\n",
        "linearly_separable = (X, y)\n",
        "\n",
        "datasets = [\n",
        "    make_moons(noise=0.3, random_state=0),\n",
        "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
        "    linearly_separable,\n",
        "]\n",
        "\n",
        "figure = plt.figure(figsize=(16, 9))\n",
        "i = 1\n",
        "# iterate over datasets\n",
        "for ds_cnt, ds in enumerate(datasets):\n",
        "    # preprocess dataset, split into training and test part\n",
        "    X, y = ds\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42\n",
        "    )\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "\n",
        "    # just plot the dataset first\n",
        "    cm = plt.cm.RdBu\n",
        "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "    if ds_cnt == 0:\n",
        "        ax.set_title(\"Input data\")\n",
        "    # Plot the training points\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
        "    # Plot the testing points\n",
        "    ax.scatter(\n",
        "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
        "    )\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    i += 1\n",
        "\n",
        "    # iterate over classifiers\n",
        "    for name, clf in zip(names, classifiers):\n",
        "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "\n",
        "        clf = make_pipeline(StandardScaler(), clf)\n",
        "        clf.fit(X_train, y_train)\n",
        "        score = clf.score(X_test, y_test)\n",
        "        DecisionBoundaryDisplay.from_estimator(\n",
        "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
        "        )\n",
        "\n",
        "        # Plot the training points\n",
        "        ax.scatter(\n",
        "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
        "        )\n",
        "        # Plot the testing points\n",
        "        ax.scatter(\n",
        "            X_test[:, 0],\n",
        "            X_test[:, 1],\n",
        "            c=y_test,\n",
        "            cmap=cm_bright,\n",
        "            edgecolors=\"k\",\n",
        "            alpha=0.6,\n",
        "        )\n",
        "\n",
        "        ax.set_xlim(x_min, x_max)\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "        if ds_cnt == 0:\n",
        "            ax.set_title(name)\n",
        "        ax.text(\n",
        "            x_max - 0.3,\n",
        "            y_min + 0.3,\n",
        "            (\"%.2f\" % score).lstrip(\"0\"),\n",
        "            size=15,\n",
        "            horizontalalignment=\"right\",\n",
        "        )\n",
        "        i += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59O6HtCqgOHT"
      },
      "source": [
        "# CNN Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5eiwFPIgOHU"
      },
      "source": [
        "Let's buil a classifier from the [CIFAR-10 dataset](https://paperswithcode.com/dataset/cifar-10 \"Canadian Institute for Advanced Research, 10 classes\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDvmaiJ9gOHU"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mduCc3WDgOHV"
      },
      "outputs": [],
      "source": [
        "# Let's see the device available on the machine\n",
        "# For a macbook with  Mx processor, use the second line\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('mps')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otecKrsagOHV"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A74Kj_KgOHW"
      },
      "outputs": [],
      "source": [
        "# Let's show some of the training images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivkoTuICgOHW"
      },
      "outputs": [],
      "source": [
        "# Define the convolutional network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ThYagz9gOHX"
      },
      "outputs": [],
      "source": [
        "# Define the losso function\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8F_6TOdgOHX"
      },
      "outputs": [],
      "source": [
        "# Train the network\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Di9fhf4gOHY"
      },
      "outputs": [],
      "source": [
        "# Let's save our trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-0biufKgOHY"
      },
      "outputs": [],
      "source": [
        "# Test the network on the test data\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a85cNoTygOHZ"
      },
      "outputs": [],
      "source": [
        "# To load a trained model just use the following\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH, weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VXqAIhwgOHZ"
      },
      "outputs": [],
      "source": [
        "#Let's see what th neural network thinke these examples above are\n",
        "outputs = net(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwGP_2VJgOHa"
      },
      "outputs": [],
      "source": [
        "# Let's print them\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1L_4t1FgOHa"
      },
      "outputs": [],
      "source": [
        "# Let's see how the network performs onn the whole dataset\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaXu5idLgOHb"
      },
      "outputs": [],
      "source": [
        "# It seems that the network learnt something, let's see the accuracy for each class\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDEiIZHwgOHb"
      },
      "source": [
        "# Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7SnxhI8gOHc"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1EzRDN7gOHc"
      },
      "outputs": [],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = \"path_to_data/celeba\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhMgUeM5gOHc"
      },
      "outputs": [],
      "source": [
        "# We can use an image folder dataset the way we have it setup.\n",
        "# Create the dataset\n",
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HhJ77RugOHg"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on ``netG`` and ``netD``\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sgHRqVbgOHg"
      },
      "outputs": [],
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*8) x 4 x 4``\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*4) x 8 x 8``\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*2) x 16 x 16``\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf) x 32 x 32``\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. ``(nc) x 64 x 64``\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9W2mm85gOHh"
      },
      "outputs": [],
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the ``weights_init`` function to randomly initialize all weights\n",
        "#  to ``mean=0``, ``stdev=0.02``.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6UGYMVFgOHi"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is ``(nc) x 64 x 64``\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf) x 32 x 32``\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*2) x 16 x 16``\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*4) x 8 x 8``\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*8) x 4 x 4``\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1crNhuugOHi"
      },
      "outputs": [],
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# Apply the ``weights_init`` function to randomly initialize all weights\n",
        "# like this: ``to mean=0, stdev=0.2``.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxS3tK6dgOHj"
      },
      "outputs": [],
      "source": [
        "# Initialize the ``BCELoss`` function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPYqfJEcgOHk"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs7Ji5jOgOHk"
      },
      "outputs": [],
      "source": [
        "# np.save(\"./img_list.npy\", img_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qh1tIe8gOHl"
      },
      "outputs": [],
      "source": [
        "img_list_saved = np.load(\"./img_list.npy\")\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list_saved]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XgIEPkbgOHm"
      },
      "outputs": [],
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjYDRwSTgOHn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comp_astr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}